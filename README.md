# ðŸš€ Local RAG Server for "Naturalâ€‘Language â†’ SQL" (WindowsÂ 11 + WSLÂ 2 + RTXÂ 3060)

> **Goal** â€“ Run a secure, local Retrievalâ€‘Augmented Generation (RAG) service that converts naturalâ€‘language questions into MySQLÂ 8.0 SQL using **OllamaÂ +Â LangChainÂ +Â Chroma**.

---

## 0. å‰ææ¡ä»¶ Â (Prerequisites)

| å¿…é ˆ                       | Version / å‚™è€ƒ                                            |
| -------------------------- | --------------------------------------------------------- |
| WindowsÂ 11Â 22H2+           | GPU ãƒ‘ã‚¹ã‚¹ãƒ«ãƒ¼ãŒå®‰å®šã™ã‚‹æœ€æ–°ç‰ˆæŽ¨å¥¨                        |
| WSLÂ 2                      | äº‹å‰ã« `wsl --install -d Ubuntu-22.04` æ¸ˆã¿               |
| NVIDIA GeForce RTXÂ 3060    | VRAM 12Â GBã€‚Compute CapabilityÂ 8.6                        |
| NVIDIAÂ Driver **â‰¥Â 551.xx** | Windows ç”¨ GameÂ Ready / Studio (CUDAÂ 12.6 ä¸–ä»£)           |
| DockerÂ DesktopÂ 4.30+       | WSLÂ 2 backend ã‚’ä½¿ç”¨ (`Settings â†’ General â†’ Use WSLÂ 2 â€¦`) |
| PythonÂ 3.10+               | `sudo apt install python3-venv`                           |

> ðŸ“ 12.6 ãƒ‰ãƒ©ã‚¤ãƒãªã‚‰ CUDA 12.6 ç³»ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ã€12.8 ãƒ‰ãƒ©ã‚¤ãƒãªã‚‰ CUDAÂ 12.8 ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ pull ã—ã¦ãã ã•ã„ã€‚

---

## 1. GPU & WSLÂ 2 å‹•ä½œç¢ºèª

```powershell
# WindowsÂ PowerShell
nvidia-smi                       # ãƒ‰ãƒ©ã‚¤ãƒã¨ CUDA Version=12.6 ã‚’ç¢ºèª
wsl --update && wsl --shutdown   # WSL ã‚«ãƒ¼ãƒãƒ«ã‚’æœ€æ–°åŒ–
```

```bash
# WSL (Ubuntu) å´
a) GPU ãŒè¦‹ãˆã‚‹ã‹ç¢ºèª
$ nvidia-smi                      # GPU ä¸€è¦§ãŒå‡ºã‚Œã° OK
```

---

## 2. Docker Desktop + GPU Validation

```powershell
# CUDAÂ 12.6 ç³»ã‚¤ãƒ¡ãƒ¼ã‚¸ä¾‹ (ãƒ‰ãƒ©ã‚¤ãƒ 12.6 ä¸–ä»£ã«åˆã‚ã›ã‚‹)
docker run --rm --gpus all \
  nvidia/cuda:12.6.2-base-ubuntu22.04 nvidia-smi  # GPU æƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œã‚Œã° OK
```

> **ã‚¿ã‚°ã®æ„å‘³** â€“ `12.6.2-base-ubuntu22.04` = CUDAÂ 12.6.2 + æœ€å° OS ã‚¤ãƒ¡ãƒ¼ã‚¸ã€‚ãƒ‰ãƒ©ã‚¤ãƒã‚‚ 12.6 ç³»ãªã‚‰äº’æ›æ€§ OKã€‚

---

## 3. Ollama ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« & ãƒ¢ãƒ‡ãƒ«å–å¾—

```bash
# Ubuntu (WSL) å´ â€“ GPU ã‚’è‡ªå‹•æ¤œçŸ¥
curl -fsSL https://ollama.com/install.sh | sh      # ðŸ—’ï¸ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©
ollama pull llama3:8b                              # ç´„ 4.6Â GB / 3Â åˆ†ã»ã©
ollama run llama3                                  # è©¦ã—èµ·å‹• (Ctrl+C ã§çµ‚äº†)
```

---

## 4. Python ä»®æƒ³ç’°å¢ƒ & ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

```bash
sudo apt update && sudo apt install -y python3-venv build-essential
python3.10 -m venv rag-env && source rag-env/bin/activate

# requirements
pip install --upgrade \
    langchain-core langchain-community langchain-text-splitters \
    chromadb sentence-transformers ollama \
    fastapi \
    'uvicorn[standard]'
```

| ãƒ©ã‚¤ãƒ–ãƒ©ãƒª                   | å½¹å‰²                                              |
| ---------------------------- | ------------------------------------------------- |
| **langchainâ€‘core/community** | RAG ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…                              |
| **langchainâ€‘textâ€‘splitters** | DDL ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«å˜ä½ã«åˆ†å‰²                          |
| **chromadb**                 | ãƒ™ã‚¯ãƒˆãƒ« DB (ãƒ­ãƒ¼ã‚«ãƒ«)                            |
| **sentenceâ€‘transformers**    | å°åž‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—                        |
| **ollama**                   | Python ã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ« LLM ã‚’å‘¼ã¶ãƒ©ãƒƒãƒ‘ãƒ¼            |
| **fastapi**                  | Python ã§ API ã‚’é–‹ç™ºã™ã‚‹ãŸã‚ã® Web ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ |
| **uvicorn[standard]**        | FastAPI èµ·å‹•ç”¨                                    |

---

## 5. DDL ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–

```bash
mkdir scripts && cd scripts
nano index_ddl.py     # ä¸‹è¨˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’è²¼ã‚Šä»˜ã‘ (Ctrl+O, Ctrl+X ã§ä¿å­˜)
python index_ddl.py   # â†’ Indexed xx chunks ã¨å‡ºãŸã‚‰æˆåŠŸ
```

```python
"""index_ddl.py â€“ CREATE TABLE ç¾¤ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–Â (è¦: schema/all_tables.sql)"""
from pathlib import Path
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from chromadb import Client

DDL = "../schema/all_tables.sql"  # DDL ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
text = Path(DDL).read_text()

splitter = RecursiveCharacterTextSplitter(
    chunk_size=800, chunk_overlap=0, separators=["CREATE TABLE", ";"],
)
docs = splitter.create_documents([text])

embed = OllamaEmbeddings(model="nomic-embed-text")
chroma = Client().create_collection("ddl")

for i, d in enumerate(docs):
    chroma.add(ids=[f"ddl_{i}"], documents=[d.page_content],
               embeddings=[embed.embed_query(d.page_content)],
               metadatas=[{"table_idx": i}])
print(f"Indexed {len(docs)} chunks")
```

---

## 6. FastAPI ã§ RAG ã‚µãƒ¼ãƒèµ·å‹•

```bash
# app.py ã‚’ä½œæˆ
nano app.py
uvicorn app:app --host 0.0.0.0 --port 8000 --workers 2  # èµ·å‹•
```

> `app.py` ã®ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã¯ `docs/app.py.sample` ã‚’å‚ç…§ (README ä¸Šéƒ¨ã«è²¼ã£ãŸã‚‚ã®ã¨åŒç­‰)ã€‚

### å‹•ä½œãƒ†ã‚¹ãƒˆ

```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "30æ­³ä»¥ä¸Šã®ç¤¾å“¡IDã¨åå‰ã‚’å–å¾—"}' | jq
```

å‡ºåŠ›ä¾‹:

```json
{
  "sql": "SELECT employee_id, name FROM employees WHERE age >= 30;",
  "ddl_context": "CREATE TABLE `employees` â€¦"
}
```

---

## 7. ã‚ˆãä½¿ã†ãƒ¡ãƒ³ãƒ†ã‚³ãƒžãƒ³ãƒ‰ (ã‚³ãƒ¡ãƒ³ãƒˆä»˜ã)

```bash
# GPU ä½¿ç”¨çŽ‡ç¢ºèª
watch -n 1 nvidia-smi        # 1ç§’ã”ã¨ã«åˆ©ç”¨çŠ¶æ³ã‚’ç¢ºèª

# Ollama ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
ollama list                  # å…¨ Pull æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«

# DDL æ›´æ–° â†’ å†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (CI ã§è‡ªå‹•åŒ–æŽ¨å¥¨)
python scripts/index_ddl.py

# RAG ã‚µãƒ¼ãƒã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹• (tmux)
tmux new -s rag "uvicorn app:app --host 0.0.0.0 --port 8000 --workers 2"
```

---

> Â© 2025 â€“ Internal use only. Powered by Ollama, LangChain, Chroma, FastAPI.
